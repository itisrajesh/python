{"cells":[{"cell_type":"markdown","metadata":{"id":"avnjgULvYV17"},"source":["# Pandas\n","\n","Pandas is a powerful Python library for data manipulation and analysis. It provides high-performance data structures and data analysis tools that make working with data efficient and easy.\n","\n","Here are some of the key reasons why you might use pandas:\n","\n","1. Data Cleaning and Preparation:\n","   * Handling missing values: Pandas offers functions to identify and handle missing data, such as filling them with specific values or removing rows or columns containing missing data.\n","   * Data normalization: You can standardize data to a common scale, which is often necessary for certain machine learning algorithms.\n","   * Data transformation: Pandas provides tools to transform data, such as converting data types, grouping data, and aggregating values.\n","\n","2. Data Analysis:\n","   * Descriptive statistics: Calculate summary statistics like mean, median, mode, standard deviation, and correlation coefficients to gain insights into your data.\n","   * Data visualization: Pandas integrates well with visualization libraries like Matplotlib and Seaborn, allowing you to create informative charts and graphs to explore your data visually.\n","   * Data filtering and querying: Easily filter data based on specific criteria and query dataframes to extract relevant information.\n","\n","3. Data Manipulation:\n","   * Data merging and joining: Combine data from multiple sources into a single dataset.\n","   * Data reshaping: Restructure dataframes, such as pivoting or stacking data, to suit your analysis needs.\n","   * Time series analysis: Pandas provides tools for working with time series data, including date and time handling, time-based indexing, and time series analysis techniques.\n","\n","4. Integration with Other Libraries:\n","   * Seaborn: Create visually appealing statistical plots with ease.\n","   * Scikit-learn: Perform machine learning tasks, such as classification, regression, and clustering.\n","   * Statsmodels: Conduct statistical modeling and hypothesis testing.\n","\n","5. Efficiency and Performance:\n","   * Optimized data structures: Pandas' data structures are designed for efficient data manipulation and analysis.\n","   * Vectorized operations: Perform operations on entire datasets at once, leading to improved performance.\n","\n","## Series\n","\n","* One-dimensional array: A Series is essentially a one-dimensional labeled array that can hold any data type (integers, floats, strings, objects, etc.).\n","* Labels: Each element in a Series is associated with a label, which can be any immutable object (e.g., integers, strings). These labels are often referred to as the Series' index.\n","* Creation:\n","\n","```python\n","import pandas as pd\n","\n","# Creating a Series from a list\n","s = pd.Series([1, 2, 3, 4])\n","\n","# Creating a Series with custom index\n","s = pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])\n","```\n","\n","## DataFrames\n","\n","* Two-dimensional labeled data structure: A DataFrame is essentially a collection of Series, where each Series represents a column and shares a common index.\n","* Rows and Columns: DataFrames are organized into rows and columns, with each row representing a record and each column representing a feature.\n","* Creation:\n","\n","```python\n","# Creating a DataFrame from a dictionary\n","data = {'col1': [1, 2, 3], 'col2': ['a', 'b', 'c']}\n","df = pd.DataFrame(data)\n","\n","# Creating a DataFrame from a list of lists\n","data = [[1, 'a'], [2, 'b'], [3, 'c']]\n","df = pd.DataFrame(data, columns=['col1', 'col2'])\n","```\n","\n","## Key Operations:\n","\n","* Selection: Accessing specific elements or subsets of data using labels or integer-based indexing.\n","* Indexing: Creating new Series or DataFrames based on specific conditions or labels.\n","* Filtering: Selecting rows or columns based on logical conditions.\n","* Aggregation: Calculating summary statistics (e.g., mean, median, sum) for Series or DataFrames.\n","* Grouping and Aggregation: Grouping data by specific columns and applying aggregate functions to each group.\n","* Joining and Merging: Combining multiple DataFrames based on common columns or indexes.\n","* Reshaping: Transforming the structure of DataFrames (e.g., pivoting, stacking, unstacking)."]},{"cell_type":"markdown","metadata":{},"source":["## Series vs DataFrame\n","\n","### Data Structures\n","\n","- Series: A one-dimensional labeled array of values.\n","- DataFrame: A two-dimensional labeled data structure with columns of potentially different types.\n","\n","\n","| Feature | Series | DataFrame |\n","|---------|--------|-----------|\n","| Dimensionality | 1-dimensional | 2-dimensional |\n","| Data structure | Labeled array | Tabular spreadsheet-like structure |\n","| Typical use | Single column of data | Multiple columns of data |\n","| Index | Single index | Row index and column labels |\n","| Data types | Can hold data of any single type | Can hold multiple data types across columns |\n","| Creation | From a list, array, or dictionary | From a dictionary of Series, list of dictionaries, or 2D numpy array |\n","| Shape | (n,) where n is the number of elements | (n, m) where n is the number of rows and m is the number of columns |\n","| Selection | Single brackets `[]` | Single `[]` or double `[[]]` brackets |\n","| Column operations | N/A (is a single column) | Can add, remove, or modify columns |\n","| Vectorized operations | Applied to entire Series | Applied to entire DataFrame or specific columns |\n","| Use case | Representing a single feature or time series | Representing a complete dataset with multiple features |\n"]},{"cell_type":"markdown","metadata":{"id":"7ufdgkMiYWUv"},"source":["## Functions\n","\n","### Series Methods\n","\n","| Category            | Method                 | Description                           | Parameters                                                                                          |\n","|---------------------|-----------------------|---------------------------------------|-----------------------------------------------------------------------------------------------------|\n","| **Creation**        | `Series()`            | Create a new Series                   | data, index, dtype                                                                                  |\n","| **Manipulation**    | `append()`            | Append values to the Series           | other, ignore_index                                                                                 |\n","|                     | `drop()`              | Drop specified labels from the Series | labels, errors                                                                                      |\n","|                     | `drop_duplicates()`   | Remove duplicate values               | keep, take_last                                                                                     |\n","|                     | `fillna()`            | Fill missing values                   | value, method, limit                                                                                |\n","|                     | `reindex()`           | Reindex the Series                    | index, method, fill_value, limit                                                                    |\n","|                     | `rename()`            | Rename the Series                     | index, axis                                                                                         |\n","| **Analysis**        | `apply()`             | Apply a function to the Series        | func, args, kwargs                                                                                  |\n","|                     | `describe()`          | Generate descriptive statistics       | percentiles, include, exclude                                                                       |\n","|                     | `unique()`            | Return the unique values              | -                                                                                                   |\n","|                     | `value_counts()`      | Return the count of unique values     | normalize, sort, ascending, bins, dropna                                                            |\n","| **Statistical**     | `max()`               | Return the maximum value              | -                                                                                                   |\n","|                     | `mean()`              | Return the mean value                 | -                                                                                                   |\n","|                     | `median()`            | Return the median value               | -                                                                                                   |\n","|                     | `min()`               | Return the minimum value              | -                                                                                                   |\n","|                     | `std()`               | Return the standard deviation         | -                                                                                                   |\n","|                     | `var()`               | Return the variance                   | -                                                                                                   |\n","| **Aggregation**     | `sum()`               | Return the sum                        | -                                                                                                   |\n","|                     | `quantile()`          | Return the quantile value             | q                                                                                                   |\n","|                     | `rank()`              | Rank the values                       | axis, method, na_option, ascending                                                                  |\n","| **Transformation**  | `astype()`            | Cast the Series to a specified dtype  | dtype                                                                                               |\n","|                     | `map()`               | Map values to a new Series            | arg                                                                                                 |\n","|                     | `interpolate()`       | Interpolate missing values            | method, axis, limit, inplace                                                                        |\n","|                     | `to_frame()`          | Convert the Series to a DataFrame     | name                                                                                                |\n","| **Input/Output**    | `to_csv()`            | Write the Series to a csv file        | path, sep, na_rep, header, index, index_label, mode                                                 |\n","|                     | `to_excel()`          | Write the Series to an excel file     | excel_writer, sheet_name, na_rep, header, index, index_label                                        |\n","|                     | `to_json()`           | Convert the Series to a json string   | path, orient, date_format, double_precision, force_ascii, date_unit, default_handler                |\n","|                     | `to_string()`         | Convert the Series to a string        | buf, na_rep, formatters, float_format, header, index, index_label                                   |\n","|                     | `to_list()`           | Convert the Series to a list          | -                                                                                                   |\n","|                     | `to_numpy()`          | Convert the Series to a numpy array   | dtype                                                                                               |\n","\n","\n","\n","---\n","\n","### DataFrame Methods\n","\n","| Category            | Method                 | Description                           | Parameters                                                                                          |\n","|---------------------|-----------------------|---------------------------------------|-----------------------------------------------------------------------------------------------------|\n","| **Creation**        | `DataFrame()`         | Create a new DataFrame                | data, index, columns, dtype                                                                         |\n","| **Manipulation**    | `append()`            | Append rows to the DataFrame          | other, ignore_index, verify_integrity                                                               |\n","|                     | `drop()`              | Drop specified labels from the DataFrame | labels, axis, errors                                                                                |\n","|                     | `drop_duplicates()`   | Remove duplicate rows                 | subset, keep, take_last, inplace                                                                     |\n","|                     | `dropna()`            | Remove missing values                 | axis, how, thresh, subset, inplace                                                                  |\n","|                     | `rename()`            | Rename the DataFrame columns          | columns                                                                                              |\n","| **Analysis**        | `apply()`             | Apply a function to the DataFrame     | func, axis, args, kwargs                                                                            |\n","|                     | `describe()`          | Generate descriptive statistics       | percentiles, include, exclude                                                                       |\n","|                     | `duplicated()`        | Return boolean Series denoting duplicate rows | subset, keep                                                                                         |\n","| **Statistical**     | `count()`             | Count the number of non-NA values     | axis, level                                                                                         |\n","|                     | `corr()`              | Compute pairwise correlation          | method, min_periods                                                                                 |\n","|                     | `cov()`               | Compute pairwise covariance           | min_periods                                                                                         |\n","| **Cumulative**      | `cummax()`            | Return the cumulative maximum         | axis, skipna                                                                                        |\n","|                     | `cummin()`            | Return the cumulative minimum         | axis, skipna                                                                                        |\n","|                     | `cumsum()`            | Return the cumulative sum             | axis, skipna                                                                                        |\n","| **Transformation**  | `astype()`            | Cast the DataFrame to a specified dtype | dtype                                                                                               |\n","|                     | `applymap()`          | Apply a function to each element      | func                                                                                                |\n","|                     | `clip()`              | Clip the values                       | lower, upper                                                                                        |\n","|                     | `combine()`           | Combine the DataFrame with another    | other, func, axis, level                                                                            |\n","|                     | `diff()`              | Compute the difference                | periods, axis                                                                                       |\n","| **Input/Output**    | `to_csv()`            | Write the DataFrame to a csv file     | path, sep, na_rep, header, index, index_label, mode                                                 |\n","|                     | `to_excel()`          | Write the DataFrame to an excel file   | excel_writer, sheet_name, na_rep, header, index, index_label                                        |\n","|                     | `to_json()`           | Convert the DataFrame to a json string | path, orient, date_format, double_precision, force_ascii, date_unit, default_handler                |\n","|                     | `to_string()`         | Convert the DataFrame to a string      | buf, na_rep, formatters, float_format, header, index, index_label                                   |\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1726031455810,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"Y7syvoydXar7","outputId":"3ee4d03a-be17-4a2d-e49d-6bfe05994bab"},"outputs":[{"name":"stdout","output_type":"stream","text":["0    10\n","1    20\n","2    30\n","3    40\n","4    50\n","dtype: int64\n","150\n","0    10\n","dtype: int64\n","a    10\n","b    20\n","c    30\n","d    40\n","e    50\n","dtype: int64\n","apple     3\n","banana    5\n","cherry    7\n","dtype: int64\n","a    10\n","b    10\n","c    10\n","dtype: int64\n","10\n","10\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_79323/1635100246.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n","  print(series[1])\n"]}],"source":["import pandas as pd\n","\n","data = [10,20,30,40,50]\n","\n","series = pd.Series(data)\n","print(series)\n","print(series.sum())\n","print(series[series <20])\n","\n","series = pd.Series(data, index=['a', 'b', 'c', 'd', 'e' ])\n","print(series)\n","\n","data_dict = {\n","    'apple' :3,\n","    'banana' :5,\n","    'cherry' :7\n","}\n","series = pd.Series(data_dict)\n","print(series)\n","\n","\n","series = pd.Series(10, index=['a', 'b', 'c'])\n","print(series)\n","print(series['b'])\n","print(series[1])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":672,"status":"ok","timestamp":1726032560193,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"gI7RZfxIfjeR","outputId":"85ea6b3e-a15f-4470-f26c-8af2209cdc24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary\n","List\n","Numpy Array\n","\n","Specific Column\n","    Name  Age\n","0   John   28\n","1   Anna   24\n","2  Peter   35\n","3  Linda   32\n","\n","Specific ROw - loc\n","Name    John\n","Age       28\n","Name: 0, dtype: object\n","\n","Specific ROw - iloc\n","Name    John\n","Age       28\n","Name: 0, dtype: object\n","\n","Filter - df[df[\"Age\"] > 30]\n","    Name  Age\n","2  Peter   35\n","3  Linda   32\n","\n","Groupyby - df[df['Age'] > 30]\n","       Age\n","Name      \n","Anna    24\n","John    28\n","Linda   32\n","Peter   35\n","\n","Sort - df.sort_values('Age')\n","    Name  Age\n","1   Anna   24\n","0   John   28\n","3  Linda   32\n","2  Peter   35\n","\n","Set index- df.set_index('Name')\n","       Age\n","Name      \n","John    28\n","Anna    24\n","Peter   35\n","Linda   32\n","\n","Mergeing - pd.merge(df, df2, on='Name')\n","    Name  Age  Country\n","0   John   28      USA\n","1   Anna   24       UK\n","2  Peter   35  Germany\n","\n","Fill NA - df.fillna(0)\n","    Name   Age\n","0   John   0.0\n","1   Anna  24.0\n","2  Peter  35.0\n","3  Linda  32.0\n","             Age\n","count   3.000000\n","mean   30.333333\n","std     5.686241\n","min    24.000000\n","25%    28.000000\n","50%    32.000000\n","75%    33.500000\n","max    35.000000\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4 entries, 0 to 3\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   Name    4 non-null      object \n"," 1   Age     3 non-null      float64\n","dtypes: float64(1), object(1)\n","memory usage: 196.0+ bytes\n","None\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","# 1. Create DataFrame from dictionary\n","data = {\n","    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n","    'Age': [28, 24, 35, 32]\n","}\n","print('Dictionary')\n","df1 = pd.DataFrame(data)  # Create DataFrame from dictionary\n","\n","# 2. Create DataFrame from list of lists\n","data = [\n","    ['John', 28], \n","    ['Anna', 24], \n","    ['Peter', 35], \n","    ['Linda', 32]\n","]\n","print('List')\n","df2 = pd.DataFrame(data, columns=['Name', 'Age'])  # Create DataFrame from list of lists\n","\n","# 3. Create DataFrame from NumPy array\n","data = np.array([['John', 28], ['Anna', 24], ['Peter', 35], ['Linda', 32]])\n","print('Numpy Array')\n","df3 = pd.DataFrame(data, columns=['Name', 'Age'])  # Create DataFrame from NumPy array\n","\n","# 4. Create DataFrame from Excel file\n","# print('Excel')\n","# df4 = pd.read_excel('data.xlsx')  # Create DataFrame from Excel file (uncomment and replace with your file path)\n","\n","# 5. Create DataFrame from CSV file\n","# print('CSV')\n","# df5 = pd.read_csv('data.csv')  # Create DataFrame from CSV file (uncomment and replace with your file path)\n","\n","# Operations on DataFrames\n","df = df1  # Use df1 for demonstration\n","\n","# 1. Select rows and columns\n","print('\\nSpecific Column')\n","print(df[['Name', 'Age']])  # Select specific columns\n","print('\\nSpecific ROw - loc')\n","print(df.loc[0])  # Select first row\n","print('\\nSpecific ROw - iloc')\n","print(df.iloc[0])  # Select first row by integer location\n","\n","# 2. Filter rows\n","print('\\nFilter - df[df[\"Age\"] > 30]')\n","print(df[df['Age'] > 30])  # Filter rows where Age > 30\n","\n","# 3. Group and aggregate\n","print(\"\\nGroupyby - df[df['Age'] > 30]\")\n","print(df.groupby('Name').sum())  # Group by Name and calculate sum of Age\n","\n","# 4. Sort and index\n","print(\"\\nSort - df.sort_values('Age')\")\n","print(df.sort_values('Age'))  # Sort by Age in ascending order\n","print(\"\\nSet index- df.set_index('Name')\")\n","print(df.set_index('Name'))  # Set Name as index\n","\n","# 5. Merge and join\n","df2 = pd.DataFrame({\n","    'Name': ['John', 'Anna', 'Peter'], \n","    'Country': ['USA', 'UK', 'Germany']}\n",")\n","print(\"\\nMergeing - pd.merge(df, df2, on='Name')\")\n","print(pd.merge(df, df2, on='Name'))  # Merge on Name column\n","\n","# 6. Handle missing data\n","df['Age'] = df['Age'].replace(28, np.nan)\n","print(\"\\nFill NA - df.fillna(0)\")\n","print(df.fillna(0))  # Fill NaN with 0\n","\n","# 7. Dataframe statistics\n","print(df.describe())  # Summary statistics\n","print(df.info())  # DataFrame information"]},{"cell_type":"markdown","metadata":{"id":"TN4DCdv4june"},"source":["### Assignment 1: ###\n","Reading rows and columns from a  CSV file\n","\n","\n","```python\n","import pandas as pd\n","\n","# 1. Basic method to read the entire CSV file\n","df1 = pd.read_csv('people_data.csv')\n","print(\"Basic Method:\\n\", df1)\n","\n","# 2. Read only specific columns from the CSV\n","df2 = pd.read_csv('people_data.csv', usecols=['Name', 'City'])\n","print(\"\\nRead Specific Columns:\\n\", df2)\n","\n","# 3. Read CSV file without a header\n","df3 = pd.read_csv('people_data.csv', header=None)\n","print(\"\\nRead Without Header:\\n\", df3)\n","\n","# 4. Read CSV file with a custom delimiter (semicolon in this case)\n","df4 = pd.read_csv('people_data.csv', sep=';')\n","print(\"\\nRead with Custom Delimiter (semicolon):\\n\", df4)\n","\n","# 5. Read large CSV files in chunks\n","chunks = pd.read_csv('people_data.csv', chunksize=100)\n","print(\"\\nReading in Chunks:\")\n","for chunk in chunks:\n","    print(chunk)\n","\n","# 6. Skip specific rows when reading the file (skipping first 2 rows here)\n","df5 = pd.read_csv('people_data.csv', skiprows=2)\n","print(\"\\nSkip First 2 Rows:\\n\", df5)\n","\n","# 7. Handling missing values by filling or replacing them\n","df6 = pd.read_csv('people_data.csv', na_values=[\"NaN\", \"None\"]).fillna(\"Unknown\")\n","print(\"\\nHandling Missing Values:\\n\", df6)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1726033311309,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"ogZZ5_YSj4CV","outputId":"cf566cfa-61ed-44a0-d44e-3a48e506dc6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["CSV file created successfully!\n","\n","    Name  Age         City Occupation\n","0   John   28     New York   Engineer\n","1   Anna   24  Los Angeles   Designer\n","2  Peter   35      Chicago     Doctor\n","3  Linda   32      Houston     Lawyer\n"]}],"source":["import csv\n","import pandas as pd\n","filename = 'people_data.csv'\n","header = ['Name', 'Age', 'City', 'Occupation']\n","data = {\n","    'Name': ['John', 'Anna', 'Peter', 'Linda'],\n","    'Age': [28, 24, 35, 32],\n","    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston'],\n","    'Occupation': ['Engineer', 'Designer', 'Doctor', 'Lawyer']\n","}\n","\n","with open(filename , mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(header)\n","    for i in range(len(data['Name'])):\n","        writer.writerow([data['Name'][i], data['Age'][i], data['City'][i], data['Occupation'][i]])\n","print(\"CSV file created successfully!\\n\")\n","\n","\n","dataframe = pd.read_csv(filename, header=0)\n","print(dataframe)"]},{"cell_type":"markdown","metadata":{"id":"8ThKGsRIn9VJ"},"source":["### Assignment 2: ###\n","\n","Create a CSV file to hold student data, roll no, name, class, marks.\n","\n","Read this data into a dictionary\n","\n","Inspect the data using head(), tail(), info(), describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1726035124181,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"fuYeidVZoSAZ","outputId":"e5b02c5e-0d1a-42f9-b88b-af3d0685dc68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Student CSV file created successfully!\n","\n","Head\n","   Roll No     Name Class  Marks\n","0      101    Alice  10th     85\n","1      102      Bob  12th     90\n","2      103  Charlie  11th     80\n","3      104    Diana  10th     88\n","4      105      Eva  12th     92\n","\n","Tail\n","   Roll No   Name Class  Marks\n","5      106  Frank  11th     80\n","6      107  Grace  10th     86\n","7      108   Hank  12th     89\n","8      109    Ivy  11th     77\n","9      110   Jack  10th     91\n","\n","Info\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10 entries, 0 to 9\n","Data columns (total 4 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   Roll No  10 non-null     int64 \n"," 1   Name     10 non-null     object\n"," 2   Class    10 non-null     object\n"," 3   Marks    10 non-null     int64 \n","dtypes: int64(2), object(2)\n","memory usage: 448.0+ bytes\n","None\n","\n","Describe \n","         Roll No      Marks\n","count   10.00000  10.000000\n","mean   105.50000  85.800000\n","std      3.02765   5.202563\n","min    101.00000  77.000000\n","25%    103.25000  81.250000\n","50%    105.50000  87.000000\n","75%    107.75000  89.750000\n","max    110.00000  92.000000\n"]}],"source":["import csv\n","import pandas as pd\n","\n","filename = 'students_data.csv'\n","students_data = {\n","    'Roll No': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eva', 'Frank', 'Grace', 'Hank', 'Ivy', 'Jack'],\n","    'Class': ['10th', '12th', '11th', '10th', '12th', '11th', '10th', '12th', '11th', '10th'],\n","    'Marks': [85, 90, 80, 88, 92, 80, 86, 89, 77, 91]\n","}\n","header = ['Roll No', 'Name', 'Class', 'Marks']\n","with open(filename , mode='w', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(header)\n","    for i in range(len(students_data['Roll No'])):\n","        writer.writerow([students_data['Roll No'][i], students_data['Name'][i], students_data['Class'][i], students_data['Marks'][i]])\n","print(\"Student CSV file created successfully!\")\n","\n","dataframe = pd.read_csv(filename, header=0)\n","dataframe['Marks']=dataframe['Marks'].astype(int)\n","print('\\nHead')\n","print(dataframe.head())\n","print('\\nTail')\n","print(dataframe.tail())\n","print('\\nInfo')\n","print(dataframe.info())\n","print('\\nDescribe ')\n","print(dataframe.describe())"]},{"cell_type":"markdown","metadata":{"id":"fg3851o4vREm"},"source":["## Outliers\n","\n","- Outliers are data points that significantly differ from other observations in a dataset\n","- They can occur due to variability in the measurement or indicate experimental errors\n","- Outliers may provide valuable insights or skew statistical analyses, depending on the context\n","\n","### Characteristics of Outliers\n","\n","- Extreme values that deviate from the pattern of the majority of the data\n","- Can be unusually high (upper outliers) or low (lower outliers)\n","- May affect measures of central tendency and dispersion\n","\n","### Outlier Detection: Interquartile Range (IQR) Method\n","\n","- Identifies outliers by defining boundaries based on the 25th percentile (Q1) and the 75th percentile (Q3)\n","- Steps to calculate:\n","  1. Calculate Q1 (25th percentile) and Q3 (75th percentile)\n","  2. Calculate IQR: IQR = Q3 - Q1\n","  3. Define lower bound: Q1 - 1.5 * IQR\n","  4. Define upper bound: Q3 + 1.5 * IQR\n","  5. Any data point outside these bounds is considered an outlier\n","\n","### Importance of Outlier Analysis\n","\n","- Helps in data cleaning and preprocessing\n","- Improves accuracy of statistical analyses and machine learning models\n","- Can reveal important anomalies or special cases in the data\n","\n","### Caution\n","\n","- Not all outliers are errors; some may represent genuine extreme values\n","- Context of the data and domain knowledge is crucial in interpreting outliers\n","- Removal of outliers should be done carefully and documented thoroughly\n","\n","### Example\n","```python\n","# Outliers Example\n","import pandas as pd\n","\n","# Sample data\n","data = {'Values': [10, 12, 12, 13, 12, 14, 10, 13, 13, 400]}\n","\n","# Create DataFrame\n","df = pd.DataFrame(data)\n","\n","# Calculate Q1, Q3, and IQR\n","Q1 = df['Values'].quantile(0.25)\n","Q3 = df['Values'].quantile(0.75)\n","IQR = Q3 - Q1\n","print('Q1 : ', Q1)\n","print('Q3 : ', Q3)\n","print('IQR : ', IQR)\n","\n","# Define bounds for outliers\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","print('lower_bound : ', lower_bound)\n","print('upper_bound : ', upper_bound)\n","\n","\n","# Identify outliers\n","outliers = df[(df['Values'] < lower_bound) | (df['Values'] > upper_bound)]\n","print(\"Outliers:\\n\", outliers)\n","```\n","\n","```console\n","Q1 :  12.0\n","Q3 :  13.0\n","IQR :  1.0\n","lower_bound :  10.5\n","upper_bound :  14.5\n","Outliers:\n","    Values\n","0      10\n","6      10\n","9     400\n","```"]},{"cell_type":"markdown","metadata":{},"source":["The rank function in pandas!\n","The rank function in pandas is used to assign ranks to the values in a Series or DataFrame. It can be used to rank the values in ascending or descending order.\n","Syntax:\n","df.rank(method='average', axis=0, na_option='keep', ascending=True)\n","Parameters:\n","\n","* method: Method to use for ranking. Options are:\n","    * average: Average rank of group (default).\n","    * min: Lowest rank of group.\n","    * max: Highest rank of group.\n","    * first: ranks assigned in order of appearance.\n","    * dense: Like 'min', but rank always increases by 1 between groups.\n","* axis: Axis to rank on. (0 or 'index' for rows, 1 or 'columns' for columns).\n","* na_option: How to handle NaN values. Options are:\n","    * keep (default): Keep NaN values as is.\n","    * top: Assign highest rank to NaN values.\n","    * bottom: Assign lowest rank to NaN values.\n","* ascending: Whether to rank in ascending (True) or descending (False) order.\n","\n","Example Use Cases:\n","\n","    Ranking values in a Series:\n","\n","```Python\n","\n","import pandas as pd\n","\n","s = pd.Series([10, 20, 30, 40, 50])\n","ranked_s = s.rank()\n","print(ranked_s)\n","```\n","Output\n","```console\n","0    1.0\n","1    2.0\n","2    3.0\n","3    4.0\n","4    5.0\n","dtype: float64\n","```\n","    Ranking values in a DataFrame:\n","\n","```Python\n","\n","df = pd.DataFrame({'A': [10, 20, 30], 'B': [40, 50, 60]})\n","ranked_df = df.rank()\n","print(ranked_df)\n","```\n","Output\n","```console\n","   A  B\n","0  1.0  1.0\n","1  2.0  2.0\n","2  3.0  3.0\n","```\n","    Ranking in descending order:\n","\n","```Python\n","\n","df = pd.DataFrame({'A': [10, 20, 30]})\n","ranked_df = df.rank(ascending=False)\n","print(ranked_df)\n","```\n","Output\n","```console\n","   A\n","0  3.0\n","1  2.0\n","2  1.0\n","```\n","    Ranking with NaN values:\n","\n","```Python\n","\n","s = pd.Series([10, np.nan, 30])\n","ranked_s = s.rank()\n","print(ranked_s)\n","```\n","Output\n","```console\n","0    1.0\n","1    NaN\n","2    2.0\n","dtype: float64\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":656,"status":"ok","timestamp":1726037496649,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"nFb-WnIOvjRW","outputId":"0cf975fb-dc32-44cf-8334-d468bab9043a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average rank :\n","0    4.5\n","1    7.0\n","2    1.0\n","3    2.5\n","4    2.5\n","5    4.5\n","6    6.0\n","dtype: float64\n","Min rank :\n","0    4.0\n","1    7.0\n","2    1.0\n","3    2.0\n","4    2.0\n","5    4.0\n","6    6.0\n","dtype: float64\n","Max rank :\n","0    5.0\n","1    7.0\n","2    1.0\n","3    3.0\n","4    3.0\n","5    5.0\n","6    6.0\n","dtype: float64\n","First rank :\n","0    4.0\n","1    7.0\n","2    1.0\n","3    2.0\n","4    3.0\n","5    5.0\n","6    6.0\n","dtype: float64\n","Dense rank :\n","0    3.0\n","1    5.0\n","2    1.0\n","3    2.0\n","4    2.0\n","5    3.0\n","6    4.0\n","dtype: float64\n"]}],"source":["import pandas as pd\n","data = pd.Series([30,60,10,20,20,30,40])\n","\n","print(\"Average rank :\")\n","print(data.rank(method=\"average\"))\n","\n","print(\"Min rank :\")\n","print(data.rank(method=\"min\"))\n","\n","print(\"Max rank :\")\n","print(data.rank(method=\"max\"))\n","\n","print(\"First rank :\")\n","print(data.rank(method=\"first\"))\n","\n","print(\"Dense rank :\")\n","print(data.rank(method=\"dense\"))"]},{"cell_type":"markdown","metadata":{"id":"MEPcj0u847bG"},"source":["### Selection in pd.DataFrame ###"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1726039464594,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"uZWXwzj55KeR","outputId":"4841da0b-de44-4319-c714-ad22931aed55"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","df\n","   Roll No     Name Class  Score\n","0      101    Alice  10th     85\n","1      102      Bob  12th     90\n","2      103  Charlie  11th     80\n","3      104    Diana  10th     88\n","4      105      Eva  12th     92\n","5      106    Frank  11th     80\n","6      107    Grace  10th     86\n","7      108     Hank  12th     89\n","8      109      Ivy  11th     77\n","9      110     Jack  10th     91\n","\n","df[['Name', 'Score']]\n","      Name  Score\n","0    Alice     85\n","1      Bob     90\n","2  Charlie     80\n","3    Diana     88\n","4      Eva     92\n","5    Frank     80\n","6    Grace     86\n","7     Hank     89\n","8      Ivy     77\n","9     Jack     91\n","\n","df.iloc[0]\n","Roll No      101\n","Name       Alice\n","Class       10th\n","Score         85\n","Name: 0, dtype: object\n","\n","df.iloc[:3]\n","   Roll No     Name Class  Score\n","0      101    Alice  10th     85\n","1      102      Bob  12th     90\n","2      103  Charlie  11th     80\n","\n","df.iloc[1:4, [0, 2]]\n","   Roll No Class\n","1      102  12th\n","2      103  11th\n","3      104  10th\n","\n","df.iloc[2]\n","Roll No        103\n","Name       Charlie\n","Class         11th\n","Score           80\n","Name: 2, dtype: object\n","\n","df.loc[df['Score'] > 90]\n","   Roll No  Name Class  Score\n","4      105   Eva  12th     92\n","9      110  Jack  10th     91\n","\n","df.loc[1:3, ['Name', 'Score']]\n","      Name  Score\n","1      Bob     90\n","2  Charlie     80\n","3    Diana     88\n","\n","df[df['Roll No'].isin([104,105,109])]\n","   Roll No   Name Class  Score\n","3      104  Diana  10th     88\n","4      105    Eva  12th     92\n","8      109    Ivy  11th     77\n","\n","df.at[0,'Score']\n","85\n","\n","df.at[1,'Name']\n","Bob\n"]}],"source":["\n","import pandas as pd\n","\n","students_data = {\n","    'Roll No': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eva', 'Frank', 'Grace', 'Hank', 'Ivy', 'Jack'],\n","    'Class': ['10th', '12th', '11th', '10th', '12th', '11th', '10th', '12th', '11th', '10th'],\n","    'Score': [85, 90, 80, 88, 92, 80, 86, 89, 77, 91]\n","}\n","\n","df = pd.DataFrame(students_data)\n","\n","print('\\ndf')\n","print(df)\n","print(\"\\ndf[['Name', 'Score']]\")\n","print(df[['Name', 'Score']])\n","print(\"\\ndf.iloc[0]\")                             \n","print(df.iloc[0])                             # Access the first row by index position\n","print(\"\\ndf.iloc[:3]\")                            \n","print(df.iloc[:3])                            # Access the first 3 rows\n","print(\"\\ndf.iloc[1:4, [0, 2]]\")                   \n","print(df.iloc[1:4, [0, 2]])                   # Access rows 1 to 3 and specific columns\n","print(\"\\ndf.iloc[2]\")                             \n","print(df.iloc[2])                             # Access row at index 2 by position\n","print(\"\\ndf.loc[df['Score'] > 90]\")               \n","print(df.loc[df['Score'] > 90])               # Filter rows where Score is greater than 90\n","print(\"\\ndf.loc[1:3, ['Name', 'Score']]\")         \n","print(df.loc[1:3, ['Name', 'Score']])         # Access a subset of rows and columns using labels\n","print(\"\\ndf[df['Roll No'].isin([104,105,109])]\")  \n","print(df[df['Roll No'].isin([104,105,109])])  # Filter based on Roll No\n","print(\"\\ndf.at[0,'Score']\")                       \n","print(df.at[0,'Score'])                       # Access a specific element by label (row 0, 'Score')\n","print(\"\\ndf.at[1,'Name']\")                        \n","print(df.at[1,'Name'])                        # Access by label (row 1, 'Name')"]},{"cell_type":"markdown","metadata":{"id":"gyVBS-FO7u20"},"source":["### Assignment ###\n","* Test data loading and retrieval to perform selection operations\n","\n","* Load dataset: display the first 5 rows to understand the data structure.\n","    * Use: pd.read_csv()\n","\n","* Select Specific Columns: Extract the columns Invoice ID, Product Line, Total, display first 10 rows.\n","\n","* Filter Data by Conditions: Select rows where the City is \"Yangon' and the Payment method is 'Ewallet. Display first 5 results.\n","\n","* Select Rows by Index Position: Display first 5 rows and the first 3 columns.\n","    * Use: iloc\n","\n","* Select Rows by Label: Display rows from index 5 to 10 and columns Invoice ID, Branch, and Gender.\n","    * Use: loc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":564,"status":"ok","timestamp":1726040523563,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"M1oA1RLZ-u1M","outputId":"4cf941b4-2a00-46cc-8051-8adb4d3194a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["#1\n","             name           phone                          email  \\\n","0  Erasmus Mathis  1-771-425-6636        lobortis.mauris@aol.edu   \n","1  Alika Delacruz  1-712-233-4154            leo@protonmail.couk   \n","2     Kane Conley  1-164-866-1122  ipsum.leo.elementum@yahoo.net   \n","3   Minerva Haley  1-211-352-0675          dui.lectus@google.edu   \n","4   Brennan Hines  (750) 757-3230       duis.gravida@outlook.com   \n","\n","                     address postalZip         region      country  \\\n","0   Ap #830-2768 Odio Avenue    824256    Amur Oblast        Chile   \n","1     863-6677 Tristique Av.    817334           Luik    Australia   \n","2  P.O. Box 485, 1574 Ac St.     36965  Waals-Brabant      Germany   \n","3           3372 Gravida Av.    922257        Limburg  New Zealand   \n","4    Ap #778-6041 Semper Rd.     77496          Cusco    Singapore   \n","\n","                                                text  numberrange currency  \n","0  nec tempus mauris erat eget ipsum. Suspendisse...            9   $69.05  \n","1  ultrices, mauris ipsum porta elit, a feugiat t...            6   $37.35  \n","2  tempus scelerisque, lorem ipsum sodales purus,...           10   $32.55  \n","3  diam eu dolor egestas rhoncus. Proin nisl sem,...            3   $54.19  \n","4  arcu ac orci. Ut semper pretium neque. Morbi q...           10   $94.98  \n","#2\n","                name           phone             country\n","0     Erasmus Mathis  1-771-425-6636               Chile\n","1     Alika Delacruz  1-712-233-4154           Australia\n","2        Kane Conley  1-164-866-1122             Germany\n","3      Minerva Haley  1-211-352-0675         New Zealand\n","4      Brennan Hines  (750) 757-3230           Singapore\n","5   Derek Richardson  1-773-602-2782             Ireland\n","6       Burke Powers  1-439-762-8644  Russian Federation\n","7   Pamela Wilkerson  1-838-225-0335                Peru\n","8        Patrick Roy  1-843-329-9375              Turkey\n","9   Shaeleigh Hanson  1-707-202-2447             Nigeria\n","10        Timon Park  (537) 183-2673              Turkey\n","#3\n","              name           phone                      email  \\\n","0   Erasmus Mathis  1-771-425-6636    lobortis.mauris@aol.edu   \n","35     Yen Hampton  1-862-638-1574            rhoncus@aol.edu   \n","36  Lunea Melendez  (818) 934-1336  pharetra.nibh@outlook.org   \n","90  Kyla Donaldson  1-457-667-5321        iaculis.enim@aol.ca   \n","\n","                             address postalZip       region country  \\\n","0           Ap #830-2768 Odio Avenue    824256  Amur Oblast   Chile   \n","35  P.O. Box 682, 5390 Mollis Street     85-82        Konya   Chile   \n","36                   706-4520 Eu Ave      3822     Vestland   Chile   \n","90                9970 Felis. Street      5622      Melilla   Chile   \n","\n","                                                 text  numberrange currency  \n","0   nec tempus mauris erat eget ipsum. Suspendisse...            9   $69.05  \n","35  Fusce fermentum fermentum arcu. Vestibulum ant...            1   $41.63  \n","36  augue, eu tempor erat neque non quam. Pellente...            1   $12.59  \n","90  nibh lacinia orci, consectetuer euismod est ar...            2   $54.35  \n","#4\n","             name           phone                          email  \\\n","0  Erasmus Mathis  1-771-425-6636        lobortis.mauris@aol.edu   \n","1  Alika Delacruz  1-712-233-4154            leo@protonmail.couk   \n","2     Kane Conley  1-164-866-1122  ipsum.leo.elementum@yahoo.net   \n","3   Minerva Haley  1-211-352-0675          dui.lectus@google.edu   \n","4   Brennan Hines  (750) 757-3230       duis.gravida@outlook.com   \n","\n","                     address postalZip  \n","0   Ap #830-2768 Odio Avenue    824256  \n","1     863-6677 Tristique Av.    817334  \n","2  P.O. Box 485, 1574 Ac St.     36965  \n","3           3372 Gravida Av.    922257  \n","4    Ap #778-6041 Semper Rd.     77496  \n","#4\n","               name           phone      country\n","0    Erasmus Mathis  1-771-425-6636        Chile\n","1    Alika Delacruz  1-712-233-4154    Australia\n","2       Kane Conley  1-164-866-1122      Germany\n","3     Minerva Haley  1-211-352-0675  New Zealand\n","4     Brennan Hines  (750) 757-3230    Singapore\n","5  Derek Richardson  1-773-602-2782      Ireland\n"]}],"source":["import pandas as pd\n","\n","filename = 'random.csv'\n","df = pd.read_csv(filename, header=0)\n","print('#1')\n","print(df.head())\n","print('#2')\n","print(df.loc[0:10, ['name', 'phone', 'country']])\n","print('#3')\n","print(df.loc[(df['country'] == 'Chile') & (df['numberrange'] < 10)].head(5))\n","print('#4')\n","print(df.iloc[0:5, 0:5])\n","print('#4')\n","print(df.loc[0:5, ['name', 'phone', 'country' ]])"]},{"cell_type":"markdown","metadata":{"id":"RIAJMVCRPjpO"},"source":["### Dataframe Manipulation ###"]},{"cell_type":"markdown","metadata":{},"source":["Here’s a table explaining the differences between the `melt()` and `pivot()` functions in Python's pandas, along with examples to clearly illustrate how they work.\n","\n","| **Aspect**          | **`melt()`**                                                                                                                                     | **`pivot()`**                                                                                                                |\n","|---------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n","| **Function Purpose** | Converts a wide DataFrame into a long format, turning columns into rows.                                                                         | Converts a long DataFrame into a wide format, turning rows into columns.                                                     |\n","| **Use Case**         | When you want to \"unpivot\" a DataFrame, i.e., convert multiple columns into a single column of values and a corresponding column for variables.  | When you want to \"pivot\" a DataFrame by turning unique row values into new columns.                                           |\n","| **Key Parameters**   | - `id_vars`: Columns to keep intact (not melted).<br>- `value_vars`: Columns to unpivot.                                                        | - `index`: Columns to keep as the index.<br>- `columns`: Column whose unique values will become new columns.<br>- `values`: Columns to fill in the new DataFrame. |\n","| **Example Input**    | {'Name': ['John', 'Anna'], 'Math': [90, 85], 'Science': [80, 95]}     | pd.DataFrame({'Name': ['John', 'Anna'], 'Subject': ['Math', 'Science'], 'Score': [90, 80]}           |\n","| **Example Operation**| pd.melt(df, id_vars=['Name'], value_vars=['Math', 'Science'], var_name='Subject', value_name='Score') | df.pivot(index='Name', columns='Subject', values='Score')```              |\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1726045300220,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"0kDCw2K9PlDW","outputId":"4f19b742-9f48-4002-b162-a883a9e374d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original DataFrame:\n","    Name  Age      City\n","0   John   28  New York\n","1   Anna   24     Paris\n","2  Peter   35    Berlin\n","3  Linda   32    London\n","\n","Selecting a single column:\n","0     John\n","1     Anna\n","2    Peter\n","3    Linda\n","Name: Name, dtype: object\n","\n","Selecting multiple columns:\n","    Name  Age\n","0   John   28\n","1   Anna   24\n","2  Peter   35\n","3  Linda   32\n","\n","Selecting rows by index:\n","Name        John\n","Age           28\n","City    New York\n","Name: 0, dtype: object\n","\n","Selecting rows by condition:\n","    Name  Age    City\n","2  Peter   35  Berlin\n","3  Linda   32  London\n","\n","Filtering rows by condition:\n","    Name  Age    City\n","2  Peter   35  Berlin\n","\n","Sorting by a single column:\n","    Name  Age      City\n","1   Anna   24     Paris\n","0   John   28  New York\n","3  Linda   32    London\n","2  Peter   35    Berlin\n","\n","Sorting by multiple columns:\n","    Name  Age      City\n","1   Anna   24     Paris\n","0   John   28  New York\n","3  Linda   32    London\n","2  Peter   35    Berlin\n","\n","Grouping by a single column:\n","City\n","Berlin      1\n","London      1\n","New York    1\n","Paris       1\n","Name: Name, dtype: int64\n","\n","Grouping by multiple columns:\n","City      Age\n","Berlin    35     1\n","London    32     1\n","New York  28     1\n","Paris     24     1\n","Name: Name, dtype: int64\n","\n","Merging two dataframes:\n","    Name  Age      City  Country\n","0   John   28  New York      USA\n","1   Anna   24     Paris   France\n","2  Peter   35    Berlin  Germany\n","3  Linda   32    London       UK\n","\n","Pivoting a dataframe:\n","           Age\n","City          \n","Berlin    35.0\n","London    32.0\n","New York  28.0\n","Paris     24.0\n","\n","Melting a dataframe:\n","    Name variable  value\n","0   John      Age     28\n","1   Anna      Age     24\n","2  Peter      Age     35\n","3  Linda      Age     32\n","\n","Filling missing values:\n","    Name   Age\n","0   John  28.0\n","1   Anna  24.0\n","2  Peter   0.0\n","3      0  32.0\n","\n","Dropping missing values:\n","   Name   Age\n","0  John  28.0\n","1  Anna  24.0\n","\n","Transposing a dataframe:\n","             0      1       2       3\n","Name      John   Anna   Peter   Linda\n","Age         28     24      35      32\n","City  New York  Paris  Berlin  London\n","\n","Renaming columns:\n","  Full Name  Age      City\n","0      John   28  New York\n","1      Anna   24     Paris\n","2     Peter   35    Berlin\n","3     Linda   32    London\n","\n","Dropping columns:\n","    Name  Age\n","0   John   28\n","1   Anna   24\n","2  Peter   35\n","3  Linda   32\n","\n","Adding new columns:\n","    Name  Age      City  Country\n","0   John   28  New York      USA\n","1   Anna   24     Paris   France\n","2  Peter   35    Berlin  Germany\n","3  Linda   32    London       UK\n","\n","Dropping duplicate rows:\n","    Name  Age      City  Country\n","0   John   28  New York      USA\n","1   Anna   24     Paris   France\n","2  Peter   35    Berlin  Germany\n","3  Linda   32    London       UK\n","\n","Resetting index:\n","    Name  Age      City  Country\n","0   John   28  New York      USA\n","1   Anna   24     Paris   France\n","2  Peter   35    Berlin  Germany\n","3  Linda   32    London       UK\n","\n","Applying a function to a column:\n","    Name  Age      City  Country\n","0   John   56  New York      USA\n","1   Anna   48     Paris   France\n","2  Peter   70    Berlin  Germany\n","3  Linda   64    London       UK\n","\n","Sorting by multiple columns:\n","    Name  Age      City  Country\n","1   Anna   48     Paris   France\n","0   John   56  New York      USA\n","3  Linda   64    London       UK\n","2  Peter   70    Berlin  Germany\n","\n","Finding unique values in a column:\n","['New York' 'Paris' 'Berlin' 'London']\n","\n","Finding value counts in a column:\n","City\n","New York    1\n","Paris       1\n","Berlin      1\n","London      1\n","Name: count, dtype: int64\n","\n","Finding missing values in a column:\n","0\n","\n","Replacing values in a column:\n","    Name  Age      City  Country\n","0   John   56  New York      USA\n","1   Anna   48     Paris   France\n","2  Peter   70    Munich  Germany\n","3  Linda   64    London       UK\n","\n","Extracting date components from a datetime column:\n","0    1\n","1    1\n","2    1\n","3    1\n","Name: Date, dtype: int32\n","\n","Merging dataframes with different columns:\n","    Name  Age      City Country_x       Date Country_y\n","0   Anna   48     Paris    France 2022-01-01    France\n","1   John   56  New York       USA 2022-01-01       USA\n","2  Linda   64    London        UK 2022-01-01        UK\n","3  Peter   70    Munich   Germany 2022-01-01   Germany\n","\n","Original dataframe:\n","    Name  Age      City  Country       Date\n","0   John   56  New York      USA 2022-01-01\n","1   Anna   48     Paris   France 2022-01-01\n","2  Peter   70    Munich  Germany 2022-01-01\n","3  Linda   64    London       UK 2022-01-01\n","\n","Reshaping data from wide to long:\n","    Name variable  value\n","0   John      Age     56\n","1   Anna      Age     48\n","2  Peter      Age     70\n","3  Linda      Age     64\n","\n","Reshaping data from long to wide:\n","    Name   Age\n","0   Anna  48.0\n","1   John  56.0\n","2  Linda  64.0\n","3  Peter  70.0\n"]}],"source":["import pandas as pd\n","\n","# Creating a sample dataframe\n","data = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n","        'Age': [28, 24, 35, 32],\n","        'City': ['New York', 'Paris', 'Berlin', 'London']}\n","df = pd.DataFrame(data)\n","\n","# Displaying the original dataframe\n","print(\"Original DataFrame:\")\n","print(df)\n","\n","# **Selecting Data**\n","\n","# Selecting a single column\n","print(\"\\nSelecting a single column:\")\n","print(df['Name'])\n","\n","# Selecting multiple columns\n","print(\"\\nSelecting multiple columns:\")\n","print(df[['Name', 'Age']])\n","\n","# Selecting rows by index\n","print(\"\\nSelecting rows by index:\")\n","print(df.loc[0])\n","\n","# Selecting rows by condition\n","print(\"\\nSelecting rows by condition:\")\n","print(df[df['Age'] > 30])\n","\n","# **Filtering Data**\n","\n","# Filtering rows by condition\n","print(\"\\nFiltering rows by condition:\")\n","print(df[df['City'] == 'Berlin'])\n","\n","# **Sorting Data**\n","\n","# Sorting by a single column\n","print(\"\\nSorting by a single column:\")\n","print(df.sort_values('Age'))\n","\n","# Sorting by multiple columns\n","print(\"\\nSorting by multiple columns:\")\n","print(df.sort_values(['Age', 'Name']))\n","\n","# **Grouping Data**\n","\n","# Grouping by a single column\n","print(\"\\nGrouping by a single column:\")\n","print(df.groupby('City')['Name'].count())\n","\n","# Grouping by multiple columns\n","print(\"\\nGrouping by multiple columns:\")\n","print(df.groupby(['City', 'Age'])['Name'].count())\n","\n","# **Merging Data**\n","\n","# Creating another dataframe\n","data2 = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n","         'Country': ['USA', 'France', 'Germany', 'UK']}\n","df2 = pd.DataFrame(data2)\n","\n","# Merging two dataframes\n","print(\"\\nMerging two dataframes:\")\n","print(pd.merge(df, df2, on='Name'))\n","\n","# **Pivoting Data**\n","\n","# Pivoting a dataframe\n","print(\"\\nPivoting a dataframe:\")\n","print(df.pivot_table(index='City', values='Age', aggfunc='mean'))\n","\n","# **Melting Data**\n","\n","# Melting a dataframe\n","print(\"\\nMelting a dataframe:\")\n","print(pd.melt(df, id_vars='Name', value_vars='Age'))\n","\n","# **Handling Missing Data**\n","\n","# Creating a dataframe with missing values\n","data3 = {'Name': ['John', 'Anna', 'Peter', None],\n","         'Age': [28, 24, None, 32]}\n","df3 = pd.DataFrame(data3)\n","\n","# Filling missing values\n","print(\"\\nFilling missing values:\")\n","print(df3.fillna(0))\n","\n","# Dropping missing values\n","print(\"\\nDropping missing values:\")\n","print(df3.dropna())\n","\n","# **Transposing Data**\n","\n","# Transposing a dataframe\n","print(\"\\nTransposing a dataframe:\")\n","print(df.T)\n","\n","# **Renaming Columns**\n","\n","# Renaming columns\n","print(\"\\nRenaming columns:\")\n","print(df.rename(columns={'Name': 'Full Name'}))\n","\n","# **Dropping Columns**\n","\n","# Dropping columns\n","print(\"\\nDropping columns:\")\n","print(df.drop('City', axis=1))\n","\n","# **Adding New Columns**\n","\n","# Adding new columns\n","print(\"\\nAdding new columns:\")\n","df['Country'] = ['USA', 'France', 'Germany', 'UK']\n","print(df)\n","\n","\n","# **Dropping Duplicate Rows**\n","\n","# Dropping duplicate rows\n","print(\"\\nDropping duplicate rows:\")\n","df.drop_duplicates(inplace=True)\n","print(df)\n","\n","# **Resetting Index**\n","\n","# Resetting index\n","print(\"\\nResetting index:\")\n","print(df.reset_index(drop=True))\n","\n","# **Applying Functions**\n","\n","# Applying a function to a column\n","print(\"\\nApplying a function to a column:\")\n","df['Age'] = df['Age'].apply(lambda x: x*2)\n","print(df)\n","\n","# **Sorting by Multiple Columns**\n","\n","# Sorting by multiple columns\n","print(\"\\nSorting by multiple columns:\")\n","print(df.sort_values(['Age', 'Name']))\n","\n","# **Finding Unique Values**\n","\n","# Finding unique values in a column\n","print(\"\\nFinding unique values in a column:\")\n","print(df['City'].unique())\n","\n","# **Finding Value Counts**\n","\n","# Finding value counts in a column\n","print(\"\\nFinding value counts in a column:\")\n","print(df['City'].value_counts())\n","\n","# **Finding Missing Values**\n","\n","# Finding missing values in a column\n","print(\"\\nFinding missing values in a column:\")\n","print(df['Age'].isnull().sum())\n","\n","# **Replacing Values**\n","\n","# Replacing values in a column\n","print(\"\\nReplacing values in a column:\")\n","df['City'] = df['City'].replace('Berlin', 'Munich')\n","print(df)\n","\n","# **Extracting Date Components**\n","\n","# Extracting date components from a datetime column\n","print(\"\\nExtracting date components from a datetime column:\")\n","df['Date'] = pd.to_datetime('2022-01-01')\n","print(df['Date'].dt.day)\n","\n","# **Merging Dataframes with Different Columns**\n","\n","# Merging dataframes with different columns\n","print(\"\\nMerging dataframes with different columns:\")\n","data4 = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n","         'Country': ['USA', 'France', 'Germany', 'UK']}\n","df4 = pd.DataFrame(data4)\n","print(pd.merge(df, df4, on='Name', how='outer'))\n","\n","# **Reshaping Data**\n","\n","# Original dataframe\n","print(\"\\nOriginal dataframe:\")\n","print(df)\n","# Reshaping data from wide to long\n","print(\"\\nReshaping data from wide to long:\")\n","print(pd.melt(df, id_vars='Name', value_vars='Age'))\n","\n","# Reshaping data from long to wide\n","print(\"\\nReshaping data from long to wide:\")\n","print(df.pivot_table(index='Name', values='Age', aggfunc='mean').reset_index())"]},{"cell_type":"markdown","metadata":{"id":"rYQYe9jrUWBA"},"source":["### Handling Missing Values ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":684,"status":"ok","timestamp":1726045649663,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"-Nyf3CMsUSS1","outputId":"41d8ce91-ae72-4291-ce01-0f805d6674df"},"outputs":[{"name":"stdout","output_type":"stream","text":["     A    B\n","0  1.0  5.0\n","1  2.0  NaN\n","2  NaN  7.0\n","3  4.0  8.0\n","1. Fill missing values with a specific value:\n","     A    B\n","0  1.0  5.0\n","1  2.0  0.0\n","2  0.0  7.0\n","3  4.0  8.0\n","\n","2. Fill missing values forward or backward:\n","     A    B\n","0  1.0  5.0\n","1  2.0  5.0\n","2  2.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  7.0\n","2  4.0  7.0\n","3  4.0  8.0\n","\n","3. Fill missing values using interpolation:\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n","\n","4. Fill missing values with custom functions:\n","          A         B\n","0  1.000000  5.000000\n","1  2.000000  6.666667\n","2  2.333333  7.000000\n","3  4.000000  8.000000\n","     A    B\n","0  1.0  5.0\n","1  2.0  5.0\n","2  1.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n","\n","5. Drop rows or columns containing missing values:\n","     A    B\n","0  1.0  5.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  NaN\n","2  NaN  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","3  4.0  8.0\n","Empty DataFrame\n","Columns: []\n","Index: [0, 1, 2, 3]\n","\n","6. Fill missing values based on conditions:\n","          A         B\n","0  1.000000  5.000000\n","1  2.000000       NaN\n","2  2.333333  2.333333\n","3  4.000000  8.000000\n","\n","7. Other methods:\n","     A    B\n","0  1.0  5.0\n","1  2.0  0.0\n","2  0.0  7.0\n","3  4.0  8.0\n","     A    B\n","0  1.0  5.0\n","1  2.0  6.0\n","2  3.0  7.0\n","3  4.0  8.0\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-93-443d7b69a5c6>:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  print(df.fillna(method='ffill'))  # Forward fill\n","<ipython-input-93-443d7b69a5c6>:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  print(df.fillna(method='bfill'))  # Backward fill\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Create a sample DataFrame with missing values\n","data = {'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]}\n","df = pd.DataFrame(data)\n","print(df)\n","\n","# Methods to handle missing values\n","df = pd.DataFrame(data)\n","print(\"1. Fill missing values with a specific value:\")\n","print(df.fillna(value=0))\n","\n","df = pd.DataFrame(data)\n","print(\"\\n2. Fill missing values forward or backward:\")\n","print(df.fillna(method='ffill'))  # Forward fill\n","print(df.fillna(method='bfill'))  # Backward fill\n","\n","df = pd.DataFrame(data)\n","print(\"\\n3. Fill missing values using interpolation:\")\n","print(df.interpolate(method='linear'))  # Linear interpolation\n","print(df.interpolate(method='quadratic'))  # Quadratic interpolation\n","print(df.interpolate(method='polynomial', order=2))  # Polynomial interpolation\n","print(df.interpolate(method='spline', order=2))  # Spline interpolation\n","\n","df = pd.DataFrame(data)\n","print(\"\\n4. Fill missing values with custom functions:\")\n","print(df.fillna(value=df.mean()))  # Mean of the column\n","print(df.fillna(value=df.mode().iloc[0]))  # Most frequent value\n","print(df.apply(lambda x: x.fillna(x.interpolate(method='spline', order=2)), axis=0))  # Custom interpolation\n","\n","df = pd.DataFrame(data)\n","print(\"\\n5. Drop rows or columns containing missing values:\")\n","print(df.dropna())  # Drop rows with missing values\n","print(df.dropna(how='all'))  # Drop rows where all values are missing\n","print(df.dropna(how='any'))  # Drop rows where any values are missing\n","print(df.dropna(axis=1))  # Drop columns with missing values\n","\n","df = pd.DataFrame(data)\n","print(\"\\n6. Fill missing values based on conditions:\")\n","condition = df['A'].isnull()\n","df_filled = df.where(~condition, other=df['A'].mean())  # Replace missing values with the mean\n","print(df_filled)\n","\n","df = pd.DataFrame(data)\n","print(\"\\n7. Other methods:\")\n","print(df.replace(to_replace=np.nan, value=0))  # Replace missing values with a specific value\n","print(df.interpolate(method='linear', limit=2))  # Interpolate missing values with a limit"]},{"cell_type":"markdown","metadata":{"id":"WprlRi96XGpz"},"source":["### Assignment ###\n","Cleaning Vaccination dataset. Use \"country_vaccinations.csv\" from Kaggle\n","\n","* Load Dataset: Display the first 5 rows to get an overview of the data structure. Use: pd.read_csv()\n","\n","* Add new column 'vaccination_rate' percentage of population vaccinated with at least one dose (use a constant population estimate)\n","\n","* Modify 'daily_vaccinations' column by filling any missing values with mean of column.\n","\n","* Delete 'source_name' column\n","\n","* Sort DataFrame by 'total_vaccinations' in descending order to find countries with highest number of vaccinations.\n","\n","* Sort DataFrame by country and date to organize data by country and chronological order\n","\n","* Rename column 'iso_code' to 'country_code'\n","\n","* Rename 'index' to start from 1 instead of 0.\n","\n","\n","* Handling Missing Data:\n","\n","    * Identify columns with missing data and count the number of missing values in each column.\n","    \n","    * Use fillna() to fill missing values in 'people_fully_vaccinated' with 0, assuming no data indicates no full vaccinations.\n","    \n","    * Drop any rows where 'total_vaccinations' is missing.\n","    \n","* Replace any occurrences of \"Moderna, Pfizer/BioNTech\" in the vaccines column with \"mRNA vaccines\".\n","\n","* Replace negative values in 'daily_vaccinations' (if any) with 0, assuming these are data entry errors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1726063099560,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"yJo685bFalQB","outputId":"dfe65e0e-7ab8-4cef-96f3-446b6129b610"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['country', 'iso_code', 'date', 'total_vaccinations',\n","       'people_vaccinated', 'people_fully_vaccinated',\n","       'daily_vaccinations_raw', 'daily_vaccinations',\n","       'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',\n","       'people_fully_vaccinated_per_hundred', 'daily_vaccinations_per_million',\n","       'vaccines', 'source_name', 'source_website'],\n","      dtype='object')\n","       country iso_code        date  total_vaccinations  people_vaccinated  \\\n","0  Afghanistan      AFG  2021-02-22                 0.0                0.0   \n","1  Afghanistan      AFG  2021-02-23                 NaN                NaN   \n","2  Afghanistan      AFG  2021-02-24                 NaN                NaN   \n","3  Afghanistan      AFG  2021-02-25                 NaN                NaN   \n","4  Afghanistan      AFG  2021-02-26                 NaN                NaN   \n","\n","   people_fully_vaccinated  daily_vaccinations_raw  daily_vaccinations  \\\n","0                      NaN                     NaN                 NaN   \n","1                      NaN                     NaN              1367.0   \n","2                      NaN                     NaN              1367.0   \n","3                      NaN                     NaN              1367.0   \n","4                      NaN                     NaN              1367.0   \n","\n","   total_vaccinations_per_hundred  people_vaccinated_per_hundred  \\\n","0                             0.0                            0.0   \n","1                             NaN                            NaN   \n","2                             NaN                            NaN   \n","3                             NaN                            NaN   \n","4                             NaN                            NaN   \n","\n","   people_fully_vaccinated_per_hundred  daily_vaccinations_per_million  \\\n","0                                  NaN                             NaN   \n","1                                  NaN                            35.0   \n","2                                  NaN                            35.0   \n","3                                  NaN                            35.0   \n","4                                  NaN                            35.0   \n","\n","                                            vaccines  \\\n","0  Johnson&Johnson, Oxford/AstraZeneca, Pfizer/Bi...   \n","1  Johnson&Johnson, Oxford/AstraZeneca, Pfizer/Bi...   \n","2  Johnson&Johnson, Oxford/AstraZeneca, Pfizer/Bi...   \n","3  Johnson&Johnson, Oxford/AstraZeneca, Pfizer/Bi...   \n","4  Johnson&Johnson, Oxford/AstraZeneca, Pfizer/Bi...   \n","\n","                 source_name            source_website  \n","0  World Health Organization  https://covid19.who.int/  \n","1  World Health Organization  https://covid19.who.int/  \n","2  World Health Organization  https://covid19.who.int/  \n","3  World Health Organization  https://covid19.who.int/  \n","4  World Health Organization  https://covid19.who.int/  \n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>daily_vaccinations</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>114971.789486</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1367.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1367.000000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1367.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1367.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31236</th>\n","      <td>18598.000000</td>\n","    </tr>\n","    <tr>\n","      <th>31237</th>\n","      <td>23205.000000</td>\n","    </tr>\n","    <tr>\n","      <th>31238</th>\n","      <td>27567.000000</td>\n","    </tr>\n","    <tr>\n","      <th>31239</th>\n","      <td>30698.000000</td>\n","    </tr>\n","    <tr>\n","      <th>31240</th>\n","      <td>33326.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31240 rows × 1 columns</p>\n","</div><br><label><b>dtype:</b> float64</label>"],"text/plain":["1        114971.789486\n","2          1367.000000\n","3          1367.000000\n","4          1367.000000\n","5          1367.000000\n","             ...      \n","31236     18598.000000\n","31237     23205.000000\n","31238     27567.000000\n","31239     30698.000000\n","31240     33326.000000\n","Name: daily_vaccinations, Length: 31240, dtype: float64"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","filename = 'country_vaccinations.csv'\n","df = pd.read_csv(filename, header=0)\n","print(df.keys())\n","print(df.head())\n","\n","df['vaccination_rate'] = (df['people_fully_vaccinated'] - df['people_fully_vaccinated']) / 200000\n","df['daily_vaccinations'] = df['daily_vaccinations'].fillna(df['daily_vaccinations'].mean())\n","df.drop('source_name', axis=1, inplace=True )\n","df.sort_values('total_vaccinations', ascending = False, inplace=True)\n","df.sort_values(['country', 'date'], inplace=True )\n","df.rename(columns={'iso_code':'country_code'}, inplace=True)\n","df.index = pd.Index(range(1, len(df) + 1))\n","df.isnull().sum()\n","df['people_fully_vaccinated'].fillna(0, inplace = True)\n","df.dropna(subset=['total_vaccinations'])\n","df['vaccines'].replace({\"Moderna, Pfizer/BioNTech\":\"mRNA vaccines\"}, inplace=True)\n","df['daily_vaccinations'].apply(lambda x:0 if x < 0 else x)"]},{"cell_type":"markdown","metadata":{"id":"NAjmyBhAowur"},"source":["### Analyzing Retail Sales Data with Pandas ###\n","\n","* Loading data and Inspection\n","\n","    * Load data from CSV file into a Pandas DataFrame.\n","    * Display the first 10 rows of the data to get an overview.\n","    * Check for any missing values and handle them appropriately.\n","\n","* Data Cleaning and Preparation:\n","\n","    * Convert the Date column to datetime format.\n","    * Remove any duplicate entries if present.\n","    * Handle outliers in the Sales column using the IQR method by replacing them with the median sales value.\n","\n","* Sales Analysis:\n","\n","    * Calculate the total sales for each store and display the top 5 stores by sales.\n","    * Find out which product category has the highest average sales.\n","    * Identify the best-selling product in terms of quantity sold.\n","\n","* Outlier Detection:\n","\n","    * Use the IQR method to detect outliers in the Quantity column.\n","\n","* Ranking Analysis:\n","\n","    * Rank the products by total sales in descending order and display the top 10 products.\n","    * Rank the stores by their average sales amount using the rank() method with the 'min' ranking method for ties.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3051,"status":"ok","timestamp":1726196354032,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"NKQCORFYMj5o","outputId":"63355e3b-1b7d-474a-c8fd-f66e861c5709"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data samples : \n","          Date    Store    Product    Category   Sales  Quantity\n","0  29-01-2023  Store_D  Product_3  Category_2  430.24        11\n","1  09-10-2023  Store_B  Product_1  Category_2  212.26        18\n","2  09-08-2023  Store_C  Product_1  Category_1  538.42         8\n","3  03-05-2023  Store_B  Product_2  Category_2  670.34         9\n","4  08-11-2023  Store_A  Product_3  Category_3  562.97        17\n","5  27-05-2023  Store_E  Product_3  Category_1  251.85        12\n","6  09-04-2023  Store_C  Product_4  Category_3  660.08         8\n","7  09-02-2023  Store_D  Product_4  Category_2  388.39         7\n","8  17-05-2023  Store_D  Product_2  Category_1  164.68         9\n","9  03-10-2023  Store_E  Product_5  Category_2  424.08         1\n","Null Values : \n"," Date        0\n","Store       0\n","Product     0\n","Category    0\n","Sales       0\n","Quantity    0\n","dtype: int64\n","Q1 :  229.6675\n","Q3 :  698.245\n","IQR :  468.5775\n","lower_bound :  -473.19875\n","upper_bound :  1401.11125\n","Outliers Removed :\n","         Date    Store    Product    Category   Sales  Quantity\n","0 2023-01-29  Store_D  Product_3  Category_2  430.24        11\n","1 2023-10-09  Store_B  Product_1  Category_2  212.26        18\n","2 2023-08-09  Store_C  Product_1  Category_1  538.42         8\n","3 2023-05-03  Store_B  Product_2  Category_2  670.34         9\n","4 2023-11-08  Store_A  Product_3  Category_3  562.97        17\n","Highest store sales :\n"," Store\n","Store_A    52090.36\n","Store_B    50726.71\n","Store_D    45760.32\n","Store_E    45513.99\n","Store_C    44523.58\n","Name: Sales, dtype: float64\n","Highest cat sales :\n"," Category_3\n","Best selling product :\n"," Product_1\n","For Quantity\n","Q1 :  6.0\n","Q3 :  15.0\n","IQR :  9.0\n","lower_bound :  -7.5\n","upper_bound :  28.5\n","TOp 10 Products :\n"," Product\n","Product_1    52281.51\n","Product_4    48749.76\n","Product_3    47888.08\n","Product_5    47410.59\n","Product_2    42285.02\n","Name: Sales, dtype: float64\n","Average sales by store :\n"," Store\n","Store_A    1.0\n","Store_B    2.0\n","Store_C    4.0\n","Store_D    3.0\n","Store_E    5.0\n","Name: Sales, dtype: float64\n"]}],"source":["import pandas as pd\n","\n","url = \"https://drive.google.com/uc?id=1D5Wrc2-ufRZwuviJKn7t8tvW13IOholf\"\n","\n","# Task 1.1\n","df  = pd.read_csv(url, header=0)\n","\n","#Task 1.2\n","print(\"Data samples : \\n\",df.head(10))\n","\"\"\"\n","      Date    Store    Product    Category   Sales  Quantity\n","0  29-01-2023  Store_D  Product_3  Category_2  430.24        11\n","1  09-10-2023  Store_B  Product_1  Category_2  212.26        18\n","2  09-08-2023  Store_C  Product_1  Category_1  538.42         8\n","3  03-05-2023  Store_B  Product_2  Category_2  670.34         9\n","4  08-11-2023  Store_A  Product_3  Category_3  562.97        17\n","\"\"\"\n","\n","#Task 1.3\n","print(\"Null Values : \\n\",df.isnull().sum())\n","df.ffill(inplace=True)\n","\n","# Task 2.1\n","df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n","\n","# Task 2.2\n","df.drop_duplicates(inplace=True)\n","\n","# Task 2.3\n","Q1 = df['Sales'].quantile(0.25)\n","Q3 = df['Sales'].quantile(0.75)\n","IQR = Q3 - Q1\n","print('Q1 : ', Q1)\n","print('Q3 : ', Q3)\n","print('IQR : ', IQR)\n","\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","print('lower_bound : ', lower_bound)\n","print('upper_bound : ', upper_bound)\n","\n","# outliers = df[(df['Sales'] < lower_bound) | (df['Sales'] > upper_bound)]\n","df.loc[(df['Sales'] < lower_bound) | (df['Sales'] > upper_bound), 'Sales'] = df['Sales'].median()\n","print(\"Outliers Removed :\\n\",df.head())\n","\n","# 3.1\n","total_sales_by_store = df.groupby('Store')['Sales'].sum()\n","top_5_stores = total_sales_by_store.sort_values(ascending=False).head(5)\n","print(\"Highest store sales :\\n\",top_5_stores)\n","\n","# 3.2\n","avg_sales_by_category = df.groupby('Category')['Sales'].mean()\n","highest_avg_sales_category = avg_sales_by_category.sort_values(ascending=False).index[0]\n","print(\"Highest cat sales :\\n\",highest_avg_sales_category)\n","\n","# 3.3\n","best_selling_product = df.groupby('Product')['Quantity'].sum().idxmax()\n","print(\"Best selling product :\\n\",best_selling_product)\n","\n","# 4.1\n","print('For Quantity')\n","Q1 = df['Quantity'].quantile(0.25)\n","Q3 = df['Quantity'].quantile(0.75)\n","IQR = Q3 - Q1\n","print('Q1 : ', Q1)\n","print('Q3 : ', Q3)\n","print('IQR : ', IQR)\n","\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","print('lower_bound : ', lower_bound)\n","print('upper_bound : ', upper_bound)\n","\n","df.loc[(df['Quantity'] < lower_bound) | (df['Quantity'] > upper_bound), 'Quantity'] = df['Quantity'].median()\n","\n","# 5.1\n","total_sales_by_product = df.groupby('Product')['Sales'].sum()\n","top_10_products = total_sales_by_product.sort_values(ascending=False).head(10)\n","print(\"TOp 10 Products :\\n\", top_10_products)\n","\n","# 5.2\n","avg_sales_by_store = df.groupby('Store')['Sales'].mean()\n","avg_sales_by_store_ranked = avg_sales_by_store.rank(method='min', ascending=False)\n","print(\"Average sales by store :\\n\", avg_sales_by_store_ranked)"]},{"cell_type":"markdown","metadata":{"id":"HYmdcwZjP7CJ"},"source":["### Hands on\n","\n","#### Assignment 1:\n","\n","Lists of employee details: Employee name, their ID, company name, and salary are stored in different lists.\n","\n","1. **Task 1**: Create the first DataFrame using the `pandas` library with the help of employee name and ID.\n","   \n","2. **Task 2**: Create the second DataFrame using the `pandas` library with the help of employee ID, company name, and salary.\n","   \n","3. **Task 3**: Generate the final DataFrame by merging the first and second DataFrames to print the employee ID, employee name, company name, and salary.\n","   - **Hint**: Use the merge operation in `pandas`."]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1284,"status":"ok","timestamp":1726657126141,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"},"user_tz":-330},"id":"MLiwWPdGxznP","outputId":"ae0a889f-bccb-4f31-cb97-8f9fd312ece3"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Employee Name  Employee ID Company Name  Salary\n","0         Alice          101    Company A   50000\n","1           Bob          102    Company B   60000\n","2       Charlie          103    Company C   70000\n"]}],"source":["import pandas as pd\n","\n","employee_names = ['Alice', 'Bob', 'Charlie']\n","employee_ids = [101, 102, 103]\n","company_names = ['Company A', 'Company B', 'Company C']\n","salaries = [50000, 60000, 70000]\n","\n","df1 = pd.DataFrame({'Employee Name': employee_names, 'Employee ID': employee_ids})\n","\n","df2 = pd.DataFrame({'Employee ID': employee_ids, 'Company Name': company_names, 'Salary': salaries})\n","\n","final_df = pd.merge(df1, df2, on='Employee ID')\n","\n","print(final_df)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO+093HT4dS8u/izQOyCIG5","mount_file_id":"1cSUdT-7nXi5iswk5E_ILfZUVk50L_3OY","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
