{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyND4RMPGncNiCBg2aUtiuqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Scrapy\n","\n","Scrapy is a powerful web scraping framework for Python. It provides a complete set of tools for extracting data from websites efficiently and structurally.\n","\n","```python\n","import scrapy\n","```\n","\n","### Core Concepts\n","\n","#### Spider\n","\n","- The main class for defining how a particular site (or group of sites) will be scraped.\n","\n","#### Item\n","\n","- Container for scraped data; defines the fields to be extracted.\n","\n","#### Item Pipeline\n","\n","- Component for processing scraped items after extraction.\n","\n","#### Selector\n","\n","- Used to extract data from HTML/XML sources using XPath or CSS expressions.\n","\n","#### Middleware\n","\n","- Hooks into Scrapy's request/response processing.\n","\n","### Common Scrapy Components\n","\n","#### Spider\n","\n","- `scrapy.Spider`: Base class for scrapy spiders\n","  - `name`: Unique identifier for the spider\n","  - `start_urls`: List of URLs where the spider will begin to crawl\n","  - `parse(response)`: Method called to handle the response downloaded for each of the requests\n","\n","#### Selectors\n","\n","- `response.css()`: Select elements using CSS selectors\n","- `response.xpath()`: Select elements using XPath expressions\n","\n","#### Items\n","\n","- `scrapy.Item`: Base class for scrapy items\n","- `scrapy.Field()`: Used to specify fields in an item\n","\n","#### Item Pipeline\n","\n","- `process_item(item, spider)`: Method to process each scraped item\n","\n","#### Settings\n","\n","- `ROBOTSTXT_OBEY`: Whether to respect robots.txt rules\n","- `CONCURRENT_REQUESTS`: The maximum number of concurrent (i.e. simultaneous) requests that will be performed by the Scrapy downloader\n","\n","Now, let's create a Python script that demonstrates various Scrapy functions. Note that Scrapy projects are typically structured as separate files and directories, but for demonstration purposes, we'll create a single script that shows the key components:\n","\n","\n","\n","```python\n","import scrapy\n","from scrapy.crawler import CrawlerProcess\n","from scrapy.item import Item, Field\n","\n","# Define Item\n","class BookItem(Item):\n","    title = Field()\n","    price = Field()\n","    rating = Field()\n","\n","# Define Spider\n","class BookSpider(scrapy.Spider):\n","    name = 'bookspider'\n","    start_urls = ['http://books.toscrape.com/']\n","\n","    def parse(self, response):\n","        for book in response.css('article.product_pod'):\n","            item = BookItem()\n","            item['title'] = book.css('h3 a::attr(title)').get()\n","            item['price'] = book.css('p.price_color::text').get()\n","            item['rating'] = book.css('p.star-rating::attr(class)').get().split()[-1]\n","            yield item\n","\n","        next_page = response.css('li.next a::attr(href)').get()\n","        if next_page is not None:\n","            yield response.follow(next_page, self.parse)\n","\n","# Define Item Pipeline\n","class PricePipeline:\n","    def process_item(self, item, spider):\n","        # Convert price to float\n","        item['price'] = float(item['price'][1:])\n","        return item\n","\n","# Define settings\n","settings = {\n","    'FEED_FORMAT': 'json',\n","    'FEED_URI': 'books.json',\n","    'ROBOTSTXT_OBEY': True,\n","    'CONCURRENT_REQUESTS': 1,\n","    'ITEM_PIPELINES': {PricePipeline: 300},\n","}\n","\n","# Set up the crawler and start the spider\n","process = CrawlerProcess(settings)\n","process.crawl(BookSpider)\n","process.start()\n","\n","print(\"Scraping completed. Check 'books.json' for results.\")\n","\n","```\n","\n","This script demonstrates several key Scrapy concepts:\n","\n","1. **Item Definition**: We define a `BookItem` class to structure the data we're scraping.\n","\n","2. **Spider**: The `BookSpider` class defines how to scrape the target website (books.toscrape.com in this case).\n","\n","3. **Parsing**: The `parse` method shows how to use CSS selectors to extract data from the HTML response.\n","\n","4. **Pagination**: We demonstrate how to follow links to the next page to scrape multiple pages.\n","\n","5. **Item Pipeline**: The `PricePipeline` class shows how to process items after they've been scraped (in this case, converting the price to a float).\n","\n","6. **Settings**: We define various settings, including output format, concurrency, and pipeline configuration.\n","\n","7. **Crawler Process**: We set up and run the crawler using `CrawlerProcess`.\n","\n","To run this script, you'll need to have Scrapy installed. You can install it using pip:\n","\n","```\n","pip install scrapy\n","```\n","\n","After running the script, it will scrape data from the books.toscrape.com website and save the results in a file named 'books.json' in the same directory as the script."],"metadata":{"id":"wxSWMyAT_gae"}},{"cell_type":"markdown","source":["### Assignment 1: Web Scraper for a single page\n"," * Install Scrapy and create a new project.\n"," * Create a spider that scrapes the title and all heading `(<h1>)` elements.\n"," * Store the scraped data in a CSV file."],"metadata":{"id":"c59nKhkwhKj2"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"g1xHofIF_duW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1726737269093,"user_tz":-330,"elapsed":11095,"user":{"displayName":"Rajesh Singh","userId":"16082860657877942970"}},"outputId":"a740662a-792b-4a41-9dcc-f587ef7ae4e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.2)\n","Requirement already satisfied: Twisted>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.7.0)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (43.0.1)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n","Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.3.1)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.9.1)\n","Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2.1)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.7.0)\n","Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.2.1)\n","Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (7.0.3)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.1)\n","Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (71.0.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1)\n","Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.2)\n","Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n","Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.17.1)\n","Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n","Requirement already satisfied: automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (24.8.1)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n","Requirement already satisfied: incremental>=24.7.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (24.7.2)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.12.2)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.10)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.32.3)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.1.0)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n","Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=18.9.0->scrapy) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.8.30)\n","Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.24.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n","Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n","Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n","Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n","Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n","Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n","Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n","Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","chromium-browser is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:scrapy.utils.log:Scrapy 2.11.2 started (bot: scrapybot)\n","2024-09-19 09:14:28 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n","INFO:scrapy.utils.log:Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n","2024-09-19 09:14:28 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n","INFO:scrapy.addons:Enabled addons:\n","[]\n","2024-09-19 09:14:28 [scrapy.addons] INFO: Enabled addons:\n","[]\n","DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n","2024-09-19 09:14:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","INFO:scrapy.extensions.telnet:Telnet Password: dfa7f7dab4416411\n","2024-09-19 09:14:28 [scrapy.extensions.telnet] INFO: Telnet Password: dfa7f7dab4416411\n","INFO:scrapy.middleware:Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2024-09-19 09:14:28 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","INFO:scrapy.crawler:Overridden settings:\n","{'DOWNLOAD_DELAY': 3,\n"," 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n","               '(KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.3'}\n","2024-09-19 09:14:28 [scrapy.crawler] INFO: Overridden settings:\n","{'DOWNLOAD_DELAY': 3,\n"," 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n","               '(KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.3'}\n","INFO:scrapy.middleware:Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2024-09-19 09:14:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","INFO:scrapy.middleware:Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2024-09-19 09:14:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","INFO:scrapy.middleware:Enabled item pipelines:\n","[]\n","2024-09-19 09:14:28 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","INFO:scrapy.core.engine:Spider opened\n","2024-09-19 09:14:28 [scrapy.core.engine] INFO: Spider opened\n","INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2024-09-19 09:14:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6024\n","2024-09-19 09:14:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n"]},{"output_type":"error","ename":"ReactorNotRestartable","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-318084f88197>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Run Spider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGoogleImagesSpider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scrapy/crawler.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0;34m\"after\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"startup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_shutdown_handlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_shutdown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mreactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstall_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_graceful_stop_reactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDeferred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/twisted/internet/base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muninstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mReactorNotRestartable\u001b[0m: "]}],"source":["# Install required libraries\n","!pip install scrapy\n","!pip install selenium\n","!apt-get install chromium-browser\n","\n","# Import necessary libraries\n","import scrapy\n","from scrapy.crawler import CrawlerProcess\n","from scrapy.selector import Selector\n","from scrapy.http import Request\n","from selenium import webdriver\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver.chrome.options import Options\n","import time\n","import csv\n","\n","# Define search query and parameters\n","search_query = \"your_search_query\"\n","num_images = 100  # Number of images to scrape\n","\n","# Construct Google Images search URL\n","url = f\"https://www.google.com/search?q={search_query}&tbm=isch&ijn=0\"\n","\n","# Create Scrapy Spider\n","class GoogleImagesSpider(scrapy.Spider):\n","    name = \"google_images\"\n","    start_urls = [url]\n","\n","    def parse(self, response):\n","        # Use Selenium to render JavaScript-heavy pages\n","        options = Options()\n","        options.add_argument(\"--headless\")\n","        options.add_argument(\"--disable-gpu\")\n","        options.add_argument(\"--window-size=1920,1080\")\n","\n","        driver = webdriver.Chrome(options=options)\n","        driver.get(response.url)\n","\n","        # Extract image URLs\n","        image_urls = []\n","        for img in driver.find_elements_by_xpath(\"//img[@class='rg_i Q4LuWd']\"):\n","            image_url = img.get_attribute(\"src\")\n","            image_urls.append(image_url)\n","            print(image_url)  # Print image URL to console\n","\n","        # Close Selenium driver\n","        driver.quit()\n","\n","        # Save image URLs to CSV file\n","        with open('image_urls.csv', 'w', newline='') as csvfile:\n","            writer = csv.writer(csvfile)\n","            writer.writerow([\"Image URL\", \"Search Query\"])  # Header\n","            for image_url in image_urls:\n","                writer.writerow([image_url, search_query])\n","\n","        # Follow pagination links (if any)\n","        try:\n","            next_page = driver.find_element_by_xpath(\"//a[@class='rnypbc']\").get_attribute(\"href\")\n","            if next_page:\n","                yield Request(next_page, callback=self.parse)\n","        except:\n","            pass\n","\n","# Create CrawlerProcess with settings\n","process = CrawlerProcess(settings={\n","    \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.3\",\n","    \"DOWNLOAD_DELAY\": 3,\n","    \"CONCURRENT_REQUESTS\": 16,\n","})\n","\n","# Run Spider\n","process.crawl(GoogleImagesSpider)\n","process.start()"]}]}